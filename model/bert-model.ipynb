{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":263780,"sourceType":"datasetVersion","datasetId":110385}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModel\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# --- Load & Prepare Dataset ---\ndata_dir = \"/kaggle/input/bbc-full-text-document-classification/bbc\"\nfolders = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\nos.chdir(data_dir)\n\nf_text, f_cat = [], []\nfor folder in folders:\n    for file in os.listdir(folder):\n        with open(os.path.join(folder, file), encoding='unicode_escape') as f:\n            f_text.append(' '.join(f.readlines()))\n            f_cat.append(folder)\n\ndf = pd.DataFrame({'news': f_text, 'category': f_cat})\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=22)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['category'], random_state=22)\n\n# --- Tokenizer & BERT Model ---\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbert_model = AutoModel.from_pretrained(model_name)\n\ndef tokenize_and_embed(texts):\n    embeddings = []\n    with torch.no_grad():\n        for text in texts:\n            inputs = tokenizer.encode_plus(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length')\n            output = bert_model(**inputs)['last_hidden_state']\n            embeddings.append(output.squeeze().numpy())  # shape: (128, 768)\n    return np.stack(embeddings)\n\n# --- Embedding with BERT ---\ntrain_embed = tokenize_and_embed(train_df['news'])\nval_embed = tokenize_and_embed(val_df['news'])\ntest_embed = tokenize_and_embed(test_df['news'])\n\n# --- Encode Labels ---\nlabel_map = {'sport': 0, 'tech': 1, 'entertainment': 2, 'politics': 3, 'business': 4}\ntrain_labels = to_categorical(train_df['category'].map(label_map), num_classes=5)\nval_labels = to_categorical(val_df['category'].map(label_map), num_classes=5)\ntest_labels = to_categorical(test_df['category'].map(label_map), num_classes=5)\n\n# --- Define Improved Model ---\ndef get_model(input_shape=(128, 768), hidden_dim=64, num_classes=5):\n    inputs = keras.Input(shape=input_shape)\n    x = layers.Bidirectional(layers.LSTM(hidden_dim, return_sequences=True))(inputs)\n    x = layers.Flatten()(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\nmodel = get_model()\n\n# --- Add EarlyStopping ---\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# --- Train Model ---\nmodel.fit(train_embed, train_labels,\n          validation_data=(val_embed, val_labels),\n          epochs=12,\n          batch_size=32,\n          callbacks=[early_stop])\n\n# --- Evaluate on Test Set ---\nloss, acc = model.evaluate(test_embed, test_labels)\nprint(f\"Test accuracy: {acc:.4f}\")\n\n# --- Classification Report ---\ny_pred = model.predict(test_embed)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_true_labels = np.argmax(test_labels, axis=1)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true_labels, y_pred_labels, target_names=label_map.keys()))\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true_labels, y_pred_labels))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"/kaggle/working/bert_text_classifier.keras\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}